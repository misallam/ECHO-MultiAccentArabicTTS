{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from transformers import logging as hf_logging\n",
    "from transformers import pipeline, logging as hf_logging\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Resample\n",
    "import numpy as np\n",
    "\n",
    "hf_logging.set_verbosity_error()\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "def main():\n",
    "    pipe = pipeline(\"audio-classification\", model=gender_model_path)\n",
    "    \n",
    "    waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "    \n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform[0].unsqueeze(0)\n",
    "    \n",
    "    if sample_rate != 16000:\n",
    "        resampler = Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    waveform_np = waveform.numpy().squeeze()\n",
    "    \n",
    "    result = pipe(waveform_np)\n",
    "    print(result[0][\"label\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file_path = r\"Gender-Classification\\Test-Dataset\\37.wav\"\n",
    "    gender_model_path = r\"Gender-Classification\\model\"\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Small Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from transformers import pipeline, logging as hf_logging\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForAudioFrameClassification\n",
    "import torch\n",
    "import librosa\n",
    "from scipy.stats import mode\n",
    "\n",
    "hf_logging.set_verbosity_error()\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "noise_base_model = r\"Noise-Detection\\base_model\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(noise_base_model)\n",
    "model = Wav2Vec2ForAudioFrameClassification.from_pretrained(noise_base_model)\n",
    "\n",
    "def read_audio(file_path, target_sr=16000):\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "    return audio\n",
    "\n",
    "def predict(audio):\n",
    "    inputs = processor(audio, return_tensors=\"pt\", sampling_rate=16000, padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    predicted_ids = torch.argmax(probabilities, dim=-1)\n",
    "    most_common = mode(predicted_ids.numpy())[0][0]\n",
    "    return model.config.id2label.get(most_common, \"Manual Review Needed\")\n",
    "\n",
    "model.config.id2label = {0: \"clean\", 1: \"noisy\"}\n",
    "\n",
    "file_path = r\"Gender-Classification\\Test-Dataset\\12.wav\"\n",
    "audio = read_audio(file_path)\n",
    "classification = predict(audio)\n",
    "print(\"The audio was classified as:\", classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Large Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from transformers import pipeline, logging as hf_logging\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForAudioFrameClassification\n",
    "import torch\n",
    "import librosa\n",
    "from scipy.stats import mode\n",
    "\n",
    "hf_logging.set_verbosity_error()\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "noise_large_model = r\"Noise-Detection\\large_model\"\n",
    "noise_large_model = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(noise_large_model)\n",
    "model = Wav2Vec2ForAudioFrameClassification.from_pretrained(noise_large_model)\n",
    "\n",
    "def read_audio(file_path, target_sr=16000):\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "    return audio\n",
    "\n",
    "def predict(audio):\n",
    "    inputs = processor(audio, return_tensors=\"pt\", sampling_rate=16000, padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    predicted_ids = torch.argmax(probabilities, dim=-1)\n",
    "    most_common = mode(predicted_ids.numpy())[0][0]\n",
    "    return model.config.id2label.get(most_common, \"Manual Review Needed\")\n",
    "\n",
    "model.config.id2label = {0: \"clean\", 1: \"noisy\"}\n",
    "\n",
    "file_path = r\"Gender-Classification\\Test-Dataset\\12.wav\"\n",
    "audio = read_audio(file_path)\n",
    "classification = predict(audio)\n",
    "print(\"The audio was classified as:\", classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from transformers import pipeline, logging as hf_logging\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForAudioFrameClassification\n",
    "import torch\n",
    "import librosa\n",
    "from scipy.stats import mode\n",
    "\n",
    "hf_logging.set_verbosity_error()\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "noise_large_model = r\"Noise-Detection\\finetuned_model\"\n",
    "\n",
    "noise_large_model = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(noise_large_model)\n",
    "model = Wav2Vec2ForAudioFrameClassification.from_pretrained(noise_large_model)\n",
    "\n",
    "def read_audio(file_path, target_sr=16000):\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "    return audio\n",
    "\n",
    "def predict(audio):\n",
    "    inputs = processor(audio, return_tensors=\"pt\", sampling_rate=16000, padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    predicted_ids = torch.argmax(probabilities, dim=-1)\n",
    "    most_common = mode(predicted_ids.numpy())[0][0]\n",
    "    return model.config.id2label.get(most_common, \"Manual Review Needed\")\n",
    "\n",
    "model.config.id2label = {0: \"clean\", 1: \"noisy\"}\n",
    "\n",
    "file_path = r\"Gender-Classification\\Test-Dataset\\12.wav\"\n",
    "audio = read_audio(file_path)\n",
    "classification = predict(audio)\n",
    "print(\"The audio was classified as:\", classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
